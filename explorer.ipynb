{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d14dbaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório de dados configurado: c:\\projects\\VDBMS\\Dados\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import polars as pl\n",
    "\n",
    "# Defina o caminho para a sua pasta \"Dados\"\n",
    "# Ajuste este caminho se a pasta \"Dados\" não estiver no mesmo diretório do notebook\n",
    "DATA_DIR = \"./Dados/\" # Certifique-se que esta pasta contém seus arquivos TSV grandes\n",
    "\n",
    "# Verificar se o diretório existe\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print(f\"Diretório '{DATA_DIR}' não encontrado. Por favor, verifique o caminho.\")\n",
    "else:\n",
    "    print(f\"Diretório de dados configurado: {os.path.abspath(DATA_DIR)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb21fda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procurando por arquivos .tsv em: c:\\projects\\VDBMS\\Dados\n",
      "\n",
      "Arquivos TSV encontrados (21):\n",
      "  1. amazon_reviews_multilingual_US_v1_00.tsv (3.38 GB)\n",
      "  2. amazon_reviews_us_Apparel_v1_00.tsv (1.84 GB)\n",
      "  3. amazon_reviews_us_Automotive_v1_00.tsv (1.26 GB)\n",
      "  4. amazon_reviews_us_Beauty_v1_00.tsv (2.00 GB)\n",
      "  5. amazon_reviews_us_Books_v1_02.tsv (3.02 GB)\n",
      "  6. amazon_reviews_us_Camera_v1_00.tsv (1.02 GB)\n",
      "  7. amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv (3.00 GB)\n",
      "  8. amazon_reviews_us_Digital_Video_Download_v1_00.tsv (1.20 GB)\n",
      "  9. amazon_reviews_us_Electronics_v1_00.tsv (1.61 GB)\n",
      "  10. amazon_reviews_us_Health_Personal_Care_v1_00.tsv (2.26 GB)\n",
      "  11. amazon_reviews_us_Mobile_Apps_v1_00.tsv (1.29 GB)\n",
      "  12. amazon_reviews_us_Music_v1_00.tsv (3.42 GB)\n",
      "  13. amazon_reviews_us_Office_Products_v1_00.tsv (1.16 GB)\n",
      "  14. amazon_reviews_us_PC_v1_00.tsv (3.40 GB)\n",
      "  15. amazon_reviews_us_Pet_Products_v1_00.tsv (1.14 GB)\n",
      "  16. amazon_reviews_us_Shoes_v1_00.tsv (1.46 GB)\n",
      "  17. amazon_reviews_us_Sports_v1_00.tsv (1.87 GB)\n",
      "  18. amazon_reviews_us_Toys_v1_00.tsv (1.83 GB)\n",
      "  19. amazon_reviews_us_Video_DVD_v1_00.tsv (3.45 GB)\n",
      "  20. amazon_reviews_us_Video_Games_v1_00.tsv (1.12 GB)\n",
      "  21. amazon_reviews_us_Wireless_v1_00.tsv (2.08 GB)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Procurando por arquivos .tsv em: {os.path.abspath(DATA_DIR)}\")\n",
    "\n",
    "# Listar apenas arquivos TSV\n",
    "tsv_files = glob.glob(os.path.join(DATA_DIR, \"*.tsv\"))\n",
    "\n",
    "if not tsv_files:\n",
    "    print(\"Nenhum arquivo .tsv encontrado na pasta 'Dados'.\")\n",
    "    print(\"Por favor, adicione seus arquivos TSV grandes à pasta.\")\n",
    "else:\n",
    "    print(f\"\\nArquivos TSV encontrados ({len(tsv_files)}):\")\n",
    "    for i, f_path in enumerate(tsv_files):\n",
    "        file_name = os.path.basename(f_path)\n",
    "        try:\n",
    "            file_size_bytes = os.path.getsize(f_path)\n",
    "            file_size_gb = file_size_bytes / (1024**3)\n",
    "            print(f\"  {i+1}. {file_name} ({file_size_gb:.2f} GB)\")\n",
    "        except OSError:\n",
    "            print(f\"  {i+1}. {file_name} (tamanho não pôde ser lido)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e23b3c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Explorando o arquivo: amazon_reviews_multilingual_US_v1_00.tsv ---\n",
      "Tentando ler as primeiras 1000 linhas para inferir o esquema...\n",
      "\n",
      "1. Nomes das Colunas:\n",
      "['marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', 'product_category', 'star_rating', 'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_headline', 'review_body', 'review_date']\n",
      "\n",
      "2. Tipos de Dados Inferidos (Esquema):\n",
      "   - Coluna: 'marketplace', Tipo Inferido: String\n",
      "   - Coluna: 'customer_id', Tipo Inferido: Int64\n",
      "   - Coluna: 'review_id', Tipo Inferido: String\n",
      "   - Coluna: 'product_id', Tipo Inferido: String\n",
      "   - Coluna: 'product_parent', Tipo Inferido: Int64\n",
      "   - Coluna: 'product_title', Tipo Inferido: String\n",
      "   - Coluna: 'product_category', Tipo Inferido: String\n",
      "   - Coluna: 'star_rating', Tipo Inferido: Int64\n",
      "   - Coluna: 'helpful_votes', Tipo Inferido: Int64\n",
      "   - Coluna: 'total_votes', Tipo Inferido: Int64\n",
      "   - Coluna: 'vine', Tipo Inferido: String\n",
      "   - Coluna: 'verified_purchase', Tipo Inferido: String\n",
      "   - Coluna: 'review_headline', Tipo Inferido: String\n",
      "   - Coluna: 'review_body', Tipo Inferido: String\n",
      "   - Coluna: 'review_date', Tipo Inferido: String\n",
      "\n",
      "3. Primeiras 5 linhas da amostra de 1000 linhas:\n",
      "shape: (5, 15)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ marketpla ┆ customer_ ┆ review_id ┆ product_i ┆ … ┆ verified_ ┆ review_he ┆ review_bo ┆ review_d │\n",
      "│ ce        ┆ id        ┆ ---       ┆ d         ┆   ┆ purchase  ┆ adline    ┆ dy        ┆ ate      │\n",
      "│ ---       ┆ ---       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│ str       ┆ i64       ┆           ┆ str       ┆   ┆ str       ┆ str       ┆ str       ┆ str      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ US        ┆ 53096384  ┆ R63J84G1L ┆ 156389011 ┆ … ┆ N         ┆ ignore    ┆ this is   ┆ 1995-08- │\n",
      "│           ┆           ┆ OX6R      ┆ 9         ┆   ┆           ┆ the       ┆ the first ┆ 13       │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆ review    ┆ 8 issues  ┆          │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆ below     ┆ of …      ┆          │\n",
      "│ US        ┆ 53096399  ┆ R1BALOA11 ┆ 155994760 ┆ … ┆ N         ┆ awesome   ┆ I've      ┆ 1995-08- │\n",
      "│           ┆           ┆ Z06MT     ┆ 8         ┆   ┆           ┆           ┆ always    ┆ 17       │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ been      ┆          │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ partial   ┆          │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ to im…    ┆          │\n",
      "│ US        ┆ 53096332  ┆ R1LLAY5W5 ┆ 067170180 ┆ … ┆ N         ┆ Read the  ┆ This is a ┆ 1995-08- │\n",
      "│           ┆           ┆ PZUS4     ┆ 0         ┆   ┆           ┆ book.     ┆ book      ┆ 30       │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆ It's      ┆ about     ┆          │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆ good.     ┆ first     ┆          │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ con…      ┆          │\n",
      "│ US        ┆ 53096335  ┆ R3R9VTJ82 ┆ 042513215 ┆ … ┆ N         ┆ Funniest  ┆ This is   ┆ 1995-09- │\n",
      "│           ┆           ┆ FXECQ     ┆ 3         ┆   ┆           ┆ book ever ┆ quite     ┆ 11       │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆ written   ┆ possibly  ┆          │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆ abo…      ┆ *the* f…  ┆          │\n",
      "│ US        ┆ 51747709  ┆ R1P5J3FNB ┆ 051712270 ┆ … ┆ N         ┆ A winner  ┆ The story ┆ 1995-10- │\n",
      "│           ┆           ┆ WTFXY     ┆ 7         ┆   ┆           ┆ that      ┆ behind    ┆ 17       │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆ didn't    ┆ the book  ┆          │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆ last.     ┆ is a…     ┆          │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆ Onl…      ┆           ┆          │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
      "\n",
      "4. Formato da amostra lida: (1000, 15) (linhas, colunas)\n",
      "\n",
      "Observações:\n",
      "- Se os tipos de dados (Dtypes) não parecerem corretos, você pode precisar:\n",
      "  a) Aumentar `N_ROWS_SAMPLE` e `infer_schema_length`.\n",
      "  b) Especificar os tipos manualmente no `pl.read_csv` usando o parâmetro `dtypes`.\n",
      "     Ex: dtypes = {'coluna_A': pl.Utf8, 'coluna_B': pl.Int64, 'coluna_data': pl.Date}\n",
      "- `ignore_errors=True` foi usado para a amostra. Para processamento completo, decida como tratar erros.\n"
     ]
    }
   ],
   "source": [
    "# Escolha um arquivo da lista acima pelo seu número (ex: 1 para o primeiro arquivo)\n",
    "# ou defina o nome do arquivo diretamente.\n",
    "# file_to_explore_name = \"seu_arquivo_grande.tsv\" # Ou use o índice\n",
    "\n",
    "FILE_INDEX = 0\n",
    "\n",
    "if not tsv_files:\n",
    "    print(\"Nenhum arquivo TSV para explorar. Execute a célula anterior.\")\n",
    "else:\n",
    "    # Vamos pegar o primeiro arquivo como exemplo, você pode mudar isso\n",
    "    selected_file_index = FILE_INDEX # Mude para o índice do arquivo que quer explorar\n",
    "    \n",
    "    if 0 <= selected_file_index < len(tsv_files):\n",
    "        file_path_to_explore = tsv_files[selected_file_index]\n",
    "        file_name_to_explore = os.path.basename(file_path_to_explore)\n",
    "        print(f\"\\n--- Explorando o arquivo: {file_name_to_explore} ---\")\n",
    "\n",
    "        # Quantas linhas ler para a amostragem inicial e inferência de esquema.\n",
    "        # Para arquivos de 1GB+, 1000-10000 linhas devem ser suficientes para uma boa inferência inicial.\n",
    "        # Ajuste conforme necessário.\n",
    "        N_ROWS_SAMPLE = 1000\n",
    "        \n",
    "        print(f\"Tentando ler as primeiras {N_ROWS_SAMPLE} linhas para inferir o esquema...\")\n",
    "\n",
    "        try:\n",
    "            # Use read_csv com n_rows para ler apenas uma parte do arquivo.\n",
    "            # separator='\\t' é crucial para arquivos TSV.\n",
    "            # infer_schema_length: Polars usa este número de linhas para inferir os tipos.\n",
    "            # Se os tipos não parecerem corretos, você pode aumentar este valor ou\n",
    "            # especificar os tipos manualmente com o parâmetro `dtypes`.\n",
    "            df_sample = pl.read_csv(\n",
    "                file_path_to_explore,\n",
    "                separator='\\t',\n",
    "                quote_char=None,\n",
    "                n_rows=N_ROWS_SAMPLE,\n",
    "                infer_schema_length=N_ROWS_SAMPLE, # Usar as mesmas N linhas para inferir\n",
    "                ignore_errors=True # Tenta pular linhas com erros de parsing na amostra\n",
    "            )\n",
    "\n",
    "            print(\"\\n1. Nomes das Colunas:\")\n",
    "            print(df_sample.columns)\n",
    "\n",
    "            print(\"\\n2. Tipos de Dados Inferidos (Esquema):\")\n",
    "            # O esquema mostra o nome da coluna e o tipo de dado inferido pelo Polars\n",
    "            for col_name, dtype in df_sample.schema.items():\n",
    "                print(f\"   - Coluna: '{col_name}', Tipo Inferido: {dtype}\")\n",
    "\n",
    "            print(f\"\\n3. Primeiras 5 linhas da amostra de {N_ROWS_SAMPLE} linhas:\")\n",
    "            print(df_sample.head(5))\n",
    "            \n",
    "            print(f\"\\n4. Formato da amostra lida: {df_sample.shape} (linhas, colunas)\")\n",
    "\n",
    "            print(\"\\nObservações:\")\n",
    "            print(\"- Se os tipos de dados (Dtypes) não parecerem corretos, você pode precisar:\")\n",
    "            print(f\"  a) Aumentar `N_ROWS_SAMPLE` e `infer_schema_length`.\")\n",
    "            print(f\"  b) Especificar os tipos manualmente no `pl.read_csv` usando o parâmetro `dtypes`.\")\n",
    "            print(f\"     Ex: dtypes = {{'coluna_A': pl.Utf8, 'coluna_B': pl.Int64, 'coluna_data': pl.Date}}\")\n",
    "            print(\"- `ignore_errors=True` foi usado para a amostra. Para processamento completo, decida como tratar erros.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao tentar ler uma amostra do arquivo {file_name_to_explore}: {e}\")\n",
    "            print(\"Verifique se o arquivo é um TSV válido e se o separador está correto.\")\n",
    "    else:\n",
    "        print(f\"Índice de arquivo inválido. Por favor, escolha um número entre 0 e {len(tsv_files)-1}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "946b2269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Usando scan_csv para análise lazy de: amazon_reviews_multilingual_US_v1_00.tsv ---\n",
      "\n",
      "1. Esquema inferido pelo scan_csv (sem carregar dados):\n",
      "   - Coluna: 'marketplace', Tipo Inferido: String\n",
      "   - Coluna: 'customer_id', Tipo Inferido: Int64\n",
      "   - Coluna: 'review_id', Tipo Inferido: String\n",
      "   - Coluna: 'product_id', Tipo Inferido: String\n",
      "   - Coluna: 'product_parent', Tipo Inferido: Int64\n",
      "   - Coluna: 'product_title', Tipo Inferido: String\n",
      "   - Coluna: 'product_category', Tipo Inferido: String\n",
      "   - Coluna: 'star_rating', Tipo Inferido: Int64\n",
      "   - Coluna: 'helpful_votes', Tipo Inferido: Int64\n",
      "   - Coluna: 'total_votes', Tipo Inferido: Int64\n",
      "   - Coluna: 'vine', Tipo Inferido: String\n",
      "   - Coluna: 'verified_purchase', Tipo Inferido: String\n",
      "   - Coluna: 'review_headline', Tipo Inferido: String\n",
      "   - Coluna: 'review_body', Tipo Inferido: String\n",
      "   - Coluna: 'review_date', Tipo Inferido: String\n",
      "\n",
      "2. Contando valores nulos por coluna (executando no arquivo inteiro):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jônatas\\AppData\\Local\\Temp\\ipykernel_29100\\1191519536.py:21: PerformanceWarning: Resolving the schema of a LazyFrame is a potentially expensive operation. Use `LazyFrame.collect_schema()` to get the schema without this warning.\n",
      "  for col_name, dtype in lazy_df.schema.items():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 15)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ marketpla ┆ customer_ ┆ review_id ┆ product_i ┆ … ┆ verified_ ┆ review_he ┆ review_bo ┆ review_d │\n",
      "│ ce        ┆ id        ┆ ---       ┆ d         ┆   ┆ purchase  ┆ adline    ┆ dy        ┆ ate      │\n",
      "│ ---       ┆ ---       ┆ u32       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│ u32       ┆ u32       ┆           ┆ u32       ┆   ┆ u32       ┆ u32       ┆ u32       ┆ u32      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 0         ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 11        ┆ 71        ┆ 0        │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
      "\n",
      "Lembre-se: operações com `lazy_df` só são executadas ao chamar `.collect()` ou `.fetch(N)`.\n"
     ]
    }
   ],
   "source": [
    "# Esta célula é opcional e demonstra como usar scan_csv para operações em arquivos grandes.\n",
    "# Requer que a Célula 3 tenha sido executada e `file_path_to_explore` esteja definido.\n",
    "\n",
    "if 'file_path_to_explore' in locals() and file_path_to_explore:\n",
    "    file_name_to_scan = os.path.basename(file_path_to_explore)\n",
    "    print(f\"\\n--- Usando scan_csv para análise lazy de: {file_name_to_scan} ---\")\n",
    "    \n",
    "    try:\n",
    "        # scan_csv não carrega os dados imediatamente, ele cria um \"plano\" (LazyFrame).\n",
    "        # Os dados são processados apenas quando você chama .collect() ou .fetch().\n",
    "        lazy_df = pl.scan_csv(\n",
    "            file_path_to_explore,\n",
    "            separator='\\t',\n",
    "            quote_char=None,\n",
    "            infer_schema_length=10000, # Pode usar um valor maior para inferência em scans\n",
    "            ignore_errors=False # Para scans completos, é melhor estar ciente dos erros\n",
    "        )\n",
    "\n",
    "        print(\"\\n1. Esquema inferido pelo scan_csv (sem carregar dados):\")\n",
    "        # O esquema é inferido com base nas primeiras `infer_schema_length` linhas.\n",
    "        for col_name, dtype in lazy_df.schema.items():\n",
    "            print(f\"   - Coluna: '{col_name}', Tipo Inferido: {dtype}\")\n",
    "\n",
    "        # Exemplo: Contar valores nulos por coluna no arquivo inteiro\n",
    "        # Esta operação será executada de forma eficiente em termos de memória.\n",
    "        print(\"\\n2. Contando valores nulos por coluna (executando no arquivo inteiro):\")\n",
    "        # null_counts = lazy_df.select(\n",
    "        #     [pl.col(c).is_null().sum().alias(f\"{c}_null_count\") for c in lazy_df.columns]\n",
    "        # ).collect()\n",
    "        # print(null_counts)\n",
    "        \n",
    "        # Uma forma mais simples de obter contagens de nulos:\n",
    "        print(lazy_df.select(pl.all().null_count()).collect())\n",
    "\n",
    "\n",
    "        # Exemplo: Obter contagens de valores para uma coluna específica (se for categórica)\n",
    "        # Substitua 'nome_da_sua_coluna_categorica' pelo nome real de uma coluna\n",
    "        # target_column_for_value_counts = 'nome_da_sua_coluna_categorica'\n",
    "        # if target_column_for_value_counts in lazy_df.columns:\n",
    "        #     print(f\"\\n3. Contagem de valores para a coluna '{target_column_for_value_counts}' (top 10):\")\n",
    "        #     value_c = lazy_df.group_by(target_column_for_value_counts).agg(\n",
    "        #         pl.count().alias(\"counts\")\n",
    "        #     ).sort(\"counts\", descending=True).limit(10).collect()\n",
    "        #     print(value_c)\n",
    "        # else:\n",
    "        #     print(f\"\\nColuna '{target_column_for_value_counts}' não encontrada para contagem de valores.\")\n",
    "            \n",
    "        print(\"\\nLembre-se: operações com `lazy_df` só são executadas ao chamar `.collect()` ou `.fetch(N)`.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao tentar escanear o arquivo {file_name_to_scan}: {e}\")\n",
    "else:\n",
    "    print(\"Execute a Célula 3 primeiro para selecionar um arquivo para exploração.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b63b84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\VDBMS\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from pymilvus import connections, utility, Collection, DataType, FieldSchema, CollectionSchema\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b69ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando modelo de embedding 'all-MiniLM-L6-v2'...\n",
      "Modelo de embedding carregado.\n",
      "Conectando ao Milvus em localhost:19530...\n",
      "Conectado ao Milvus com sucesso!\n",
      "Coleção 'reviews_data_collection' já existe.\n",
      "Carregando coleção 'reviews_data_collection' na memória...\n",
      "Coleção carregada.\n",
      "Arquivo './Dados/amazon_reviews_multilingual_US_v1_00.tsv' carregado. Total de 6931166 linhas.\n",
      "Processando e inserindo dados em lotes de 128...\n",
      "Processando lote 1/54150 (128 linhas)...\n",
      "Processando lote 2/54150 (128 linhas)...\n",
      "Processando lote 3/54150 (128 linhas)...\n",
      "Processando lote 4/54150 (128 linhas)...\n",
      "Processando lote 5/54150 (128 linhas)...\n",
      "Processando lote 6/54150 (128 linhas)...\n",
      "Processando lote 7/54150 (128 linhas)...\n",
      "Processando lote 8/54150 (128 linhas)...\n",
      "Processando lote 9/54150 (128 linhas)...\n",
      "Processando lote 10/54150 (128 linhas)...\n",
      "Processando lote 11/54150 (128 linhas)...\n",
      "Processando lote 12/54150 (128 linhas)...\n",
      "Processando lote 13/54150 (128 linhas)...\n",
      "Processando lote 14/54150 (128 linhas)...\n",
      "Processando lote 15/54150 (128 linhas)...\n",
      "Processando lote 16/54150 (128 linhas)...\n",
      "Processando lote 17/54150 (128 linhas)...\n",
      "Processando lote 18/54150 (128 linhas)...\n",
      "Processando lote 19/54150 (128 linhas)...\n",
      "Processando lote 20/54150 (128 linhas)...\n",
      "Processando lote 21/54150 (128 linhas)...\n",
      "Processando lote 22/54150 (128 linhas)...\n",
      "Processando lote 23/54150 (128 linhas)...\n",
      "Processando lote 24/54150 (128 linhas)...\n",
      "Processando lote 25/54150 (128 linhas)...\n",
      "Processando lote 26/54150 (128 linhas)...\n",
      "Processando lote 27/54150 (128 linhas)...\n",
      "Processando lote 28/54150 (128 linhas)...\n",
      "Processando lote 29/54150 (128 linhas)...\n",
      "Processando lote 30/54150 (128 linhas)...\n",
      "Processando lote 31/54150 (128 linhas)...\n",
      "Processando lote 32/54150 (128 linhas)...\n",
      "Processando lote 33/54150 (128 linhas)...\n",
      "Processando lote 34/54150 (128 linhas)...\n",
      "Processando lote 35/54150 (128 linhas)...\n",
      "Processando lote 36/54150 (128 linhas)...\n",
      "Processando lote 37/54150 (128 linhas)...\n",
      "Processando lote 38/54150 (128 linhas)...\n",
      "Processando lote 39/54150 (128 linhas)...\n",
      "Processando lote 40/54150 (128 linhas)...\n",
      "Processando lote 41/54150 (128 linhas)...\n",
      "Processando lote 42/54150 (128 linhas)...\n",
      "Processando lote 43/54150 (128 linhas)...\n",
      "Processando lote 44/54150 (128 linhas)...\n",
      "Processando lote 45/54150 (128 linhas)...\n",
      "Processando lote 46/54150 (128 linhas)...\n",
      "Processando lote 47/54150 (128 linhas)...\n",
      "Processando lote 48/54150 (128 linhas)...\n",
      "Processando lote 49/54150 (128 linhas)...\n",
      "Processando lote 50/54150 (128 linhas)...\n",
      "Processando lote 51/54150 (128 linhas)...\n",
      "Processando lote 52/54150 (128 linhas)...\n",
      "Processando lote 53/54150 (128 linhas)...\n",
      "Processando lote 54/54150 (128 linhas)...\n",
      "Processando lote 55/54150 (128 linhas)...\n",
      "Processando lote 56/54150 (128 linhas)...\n",
      "Processando lote 57/54150 (128 linhas)...\n",
      "Processando lote 58/54150 (128 linhas)...\n",
      "Processando lote 59/54150 (128 linhas)...\n",
      "Processando lote 60/54150 (128 linhas)...\n",
      "Processando lote 61/54150 (128 linhas)...\n",
      "Processando lote 62/54150 (128 linhas)...\n",
      "Processando lote 63/54150 (128 linhas)...\n",
      "Processando lote 64/54150 (128 linhas)...\n",
      "Processando lote 65/54150 (128 linhas)...\n",
      "Processando lote 66/54150 (128 linhas)...\n",
      "Processando lote 67/54150 (128 linhas)...\n",
      "Processando lote 68/54150 (128 linhas)...\n",
      "Processando lote 69/54150 (128 linhas)...\n",
      "Processando lote 70/54150 (128 linhas)...\n",
      "Processando lote 71/54150 (128 linhas)...\n",
      "Processando lote 72/54150 (128 linhas)...\n",
      "Processando lote 73/54150 (128 linhas)...\n",
      "Processando lote 74/54150 (128 linhas)...\n",
      "Processando lote 75/54150 (128 linhas)...\n",
      "Processando lote 76/54150 (128 linhas)...\n",
      "Processando lote 77/54150 (128 linhas)...\n",
      "Processando lote 78/54150 (128 linhas)...\n",
      "Processando lote 79/54150 (128 linhas)...\n",
      "Processando lote 80/54150 (128 linhas)...\n",
      "Processando lote 81/54150 (128 linhas)...\n",
      "Processando lote 82/54150 (128 linhas)...\n",
      "Processando lote 83/54150 (128 linhas)...\n",
      "Processando lote 84/54150 (128 linhas)...\n",
      "Processando lote 85/54150 (128 linhas)...\n",
      "Processando lote 86/54150 (128 linhas)...\n",
      "Processando lote 87/54150 (128 linhas)...\n",
      "Processando lote 88/54150 (128 linhas)...\n",
      "Processando lote 89/54150 (128 linhas)...\n",
      "Processando lote 90/54150 (128 linhas)...\n",
      "Processando lote 91/54150 (128 linhas)...\n",
      "Processando lote 92/54150 (128 linhas)...\n",
      "Processando lote 93/54150 (128 linhas)...\n",
      "Processando lote 94/54150 (128 linhas)...\n",
      "Processando lote 95/54150 (128 linhas)...\n",
      "Processando lote 96/54150 (128 linhas)...\n",
      "Processando lote 97/54150 (128 linhas)...\n",
      "Processando lote 98/54150 (128 linhas)...\n",
      "Processando lote 99/54150 (128 linhas)...\n",
      "Processando lote 100/54150 (128 linhas)...\n",
      "Processando lote 101/54150 (128 linhas)...\n",
      "Processando lote 102/54150 (128 linhas)...\n",
      "Processando lote 103/54150 (128 linhas)...\n",
      "Processando lote 104/54150 (128 linhas)...\n",
      "Processando lote 105/54150 (128 linhas)...\n",
      "Processando lote 106/54150 (128 linhas)...\n",
      "Processando lote 107/54150 (128 linhas)...\n",
      "Processando lote 108/54150 (128 linhas)...\n",
      "Processando lote 109/54150 (128 linhas)...\n",
      "Processando lote 110/54150 (128 linhas)...\n",
      "Processando lote 111/54150 (128 linhas)...\n",
      "Processando lote 112/54150 (128 linhas)...\n",
      "Processando lote 113/54150 (128 linhas)...\n",
      "Processando lote 114/54150 (128 linhas)...\n",
      "Processando lote 115/54150 (128 linhas)...\n",
      "Processando lote 116/54150 (128 linhas)...\n",
      "Processando lote 117/54150 (128 linhas)...\n",
      "Processando lote 118/54150 (128 linhas)...\n",
      "Processando lote 119/54150 (128 linhas)...\n",
      "Processando lote 120/54150 (128 linhas)...\n",
      "Processando lote 121/54150 (128 linhas)...\n",
      "Processando lote 122/54150 (128 linhas)...\n",
      "Processando lote 123/54150 (128 linhas)...\n",
      "Processando lote 124/54150 (128 linhas)...\n",
      "Processando lote 125/54150 (128 linhas)...\n",
      "Processando lote 126/54150 (128 linhas)...\n",
      "Processando lote 127/54150 (128 linhas)...\n",
      "Processando lote 128/54150 (128 linhas)...\n",
      "Processando lote 129/54150 (128 linhas)...\n",
      "Processando lote 130/54150 (128 linhas)...\n",
      "Processando lote 131/54150 (128 linhas)...\n",
      "Processando lote 132/54150 (128 linhas)...\n",
      "Processando lote 133/54150 (128 linhas)...\n",
      "Processando lote 134/54150 (128 linhas)...\n",
      "Processando lote 135/54150 (128 linhas)...\n",
      "Processando lote 136/54150 (128 linhas)...\n",
      "Processando lote 137/54150 (128 linhas)...\n",
      "Processando lote 138/54150 (128 linhas)...\n",
      "Processando lote 139/54150 (128 linhas)...\n",
      "Processando lote 140/54150 (128 linhas)...\n",
      "Processando lote 141/54150 (128 linhas)...\n",
      "Processando lote 142/54150 (128 linhas)...\n",
      "Processando lote 143/54150 (128 linhas)...\n",
      "Processando lote 144/54150 (128 linhas)...\n",
      "Processando lote 145/54150 (128 linhas)...\n",
      "Processando lote 146/54150 (128 linhas)...\n",
      "Processando lote 147/54150 (128 linhas)...\n",
      "Processando lote 148/54150 (128 linhas)...\n",
      "Processando lote 149/54150 (128 linhas)...\n",
      "Processando lote 150/54150 (128 linhas)...\n",
      "Processando lote 151/54150 (128 linhas)...\n",
      "Processando lote 152/54150 (128 linhas)...\n",
      "Processando lote 153/54150 (128 linhas)...\n",
      "Processando lote 154/54150 (128 linhas)...\n",
      "Processando lote 155/54150 (128 linhas)...\n",
      "Processando lote 156/54150 (128 linhas)...\n",
      "Processando lote 157/54150 (128 linhas)...\n",
      "Processando lote 158/54150 (128 linhas)...\n",
      "Processando lote 159/54150 (128 linhas)...\n",
      "Processando lote 160/54150 (128 linhas)...\n",
      "Processando lote 161/54150 (128 linhas)...\n",
      "Processando lote 162/54150 (128 linhas)...\n",
      "Processando lote 163/54150 (128 linhas)...\n",
      "Processando lote 164/54150 (128 linhas)...\n",
      "Processando lote 165/54150 (128 linhas)...\n",
      "Processando lote 166/54150 (128 linhas)...\n",
      "Processando lote 167/54150 (128 linhas)...\n",
      "Processando lote 168/54150 (128 linhas)...\n",
      "Processando lote 169/54150 (128 linhas)...\n",
      "Processando lote 170/54150 (128 linhas)...\n",
      "Processando lote 171/54150 (128 linhas)...\n",
      "Processando lote 172/54150 (128 linhas)...\n",
      "Processando lote 173/54150 (128 linhas)...\n",
      "Processando lote 174/54150 (128 linhas)...\n",
      "Processando lote 175/54150 (128 linhas)...\n",
      "Processando lote 176/54150 (128 linhas)...\n",
      "Processando lote 177/54150 (128 linhas)...\n",
      "Processando lote 178/54150 (128 linhas)...\n",
      "Processando lote 179/54150 (128 linhas)...\n",
      "Processando lote 180/54150 (128 linhas)...\n",
      "Processando lote 181/54150 (128 linhas)...\n",
      "Processando lote 182/54150 (128 linhas)...\n",
      "Processando lote 183/54150 (128 linhas)...\n",
      "Processando lote 184/54150 (128 linhas)...\n",
      "Processando lote 185/54150 (128 linhas)...\n",
      "Processando lote 186/54150 (128 linhas)...\n",
      "Processando lote 187/54150 (128 linhas)...\n",
      "Processando lote 188/54150 (128 linhas)...\n",
      "Processando lote 189/54150 (128 linhas)...\n",
      "Processando lote 190/54150 (128 linhas)...\n",
      "Processando lote 191/54150 (128 linhas)...\n",
      "Processando lote 192/54150 (128 linhas)...\n",
      "Processando lote 193/54150 (128 linhas)...\n",
      "Processando lote 194/54150 (128 linhas)...\n",
      "Processando lote 195/54150 (128 linhas)...\n",
      "Processando lote 196/54150 (128 linhas)...\n",
      "Processando lote 197/54150 (128 linhas)...\n",
      "Processando lote 198/54150 (128 linhas)...\n",
      "Processando lote 199/54150 (128 linhas)...\n",
      "Processando lote 200/54150 (128 linhas)...\n",
      "Processando lote 201/54150 (128 linhas)...\n",
      "Processando lote 202/54150 (128 linhas)...\n",
      "Processando lote 203/54150 (128 linhas)...\n",
      "Processando lote 204/54150 (128 linhas)...\n",
      "Processando lote 205/54150 (128 linhas)...\n",
      "Processando lote 206/54150 (128 linhas)...\n",
      "Processando lote 207/54150 (128 linhas)...\n",
      "Processando lote 208/54150 (128 linhas)...\n",
      "Processando lote 209/54150 (128 linhas)...\n",
      "Processando lote 210/54150 (128 linhas)...\n",
      "Processando lote 211/54150 (128 linhas)...\n",
      "Processando lote 212/54150 (128 linhas)...\n",
      "Processando lote 213/54150 (128 linhas)...\n",
      "Processando lote 214/54150 (128 linhas)...\n",
      "Processando lote 215/54150 (128 linhas)...\n",
      "Processando lote 216/54150 (128 linhas)...\n",
      "Processando lote 217/54150 (128 linhas)...\n",
      "Processando lote 218/54150 (128 linhas)...\n",
      "Processando lote 219/54150 (128 linhas)...\n",
      "Processando lote 220/54150 (128 linhas)...\n",
      "Processando lote 221/54150 (128 linhas)...\n",
      "Processando lote 222/54150 (128 linhas)...\n",
      "Processando lote 223/54150 (128 linhas)...\n",
      "Processando lote 224/54150 (128 linhas)...\n",
      "Processando lote 225/54150 (128 linhas)...\n",
      "Processando lote 226/54150 (128 linhas)...\n",
      "Processando lote 227/54150 (128 linhas)...\n",
      "Processando lote 228/54150 (128 linhas)...\n",
      "Processando lote 229/54150 (128 linhas)...\n",
      "Processando lote 230/54150 (128 linhas)...\n",
      "Processando lote 231/54150 (128 linhas)...\n",
      "Processando lote 232/54150 (128 linhas)...\n",
      "Processando lote 233/54150 (128 linhas)...\n",
      "Processando lote 234/54150 (128 linhas)...\n",
      "Processando lote 235/54150 (128 linhas)...\n",
      "Processando lote 236/54150 (128 linhas)...\n",
      "Processando lote 237/54150 (128 linhas)...\n",
      "Processando lote 238/54150 (128 linhas)...\n",
      "Processando lote 239/54150 (128 linhas)...\n",
      "Processando lote 240/54150 (128 linhas)...\n",
      "Processando lote 241/54150 (128 linhas)...\n",
      "Processando lote 242/54150 (128 linhas)...\n",
      "Processando lote 243/54150 (128 linhas)...\n",
      "Processando lote 244/54150 (128 linhas)...\n",
      "Processando lote 245/54150 (128 linhas)...\n",
      "Processando lote 246/54150 (128 linhas)...\n",
      "Processando lote 247/54150 (128 linhas)...\n",
      "Processando lote 248/54150 (128 linhas)...\n",
      "Processando lote 249/54150 (128 linhas)...\n",
      "Processando lote 250/54150 (128 linhas)...\n",
      "Processando lote 251/54150 (128 linhas)...\n",
      "Processando lote 252/54150 (128 linhas)...\n",
      "Processando lote 253/54150 (128 linhas)...\n",
      "Processando lote 254/54150 (128 linhas)...\n",
      "Processando lote 255/54150 (128 linhas)...\n",
      "Processando lote 256/54150 (128 linhas)...\n",
      "Processando lote 257/54150 (128 linhas)...\n",
      "Processando lote 258/54150 (128 linhas)...\n",
      "Processando lote 259/54150 (128 linhas)...\n",
      "Processando lote 260/54150 (128 linhas)...\n",
      "Processando lote 261/54150 (128 linhas)...\n",
      "Processando lote 262/54150 (128 linhas)...\n",
      "Processando lote 263/54150 (128 linhas)...\n",
      "Processando lote 264/54150 (128 linhas)...\n",
      "Processando lote 265/54150 (128 linhas)...\n",
      "Processando lote 266/54150 (128 linhas)...\n",
      "Processando lote 267/54150 (128 linhas)...\n",
      "Processando lote 268/54150 (128 linhas)...\n",
      "Processando lote 269/54150 (128 linhas)...\n",
      "Processando lote 270/54150 (128 linhas)...\n",
      "Processando lote 271/54150 (128 linhas)...\n",
      "Processando lote 272/54150 (128 linhas)...\n",
      "Processando lote 273/54150 (128 linhas)...\n",
      "Processando lote 274/54150 (128 linhas)...\n",
      "Processando lote 275/54150 (128 linhas)...\n",
      "Processando lote 276/54150 (128 linhas)...\n",
      "Processando lote 277/54150 (128 linhas)...\n",
      "Processando lote 278/54150 (128 linhas)...\n",
      "Processando lote 279/54150 (128 linhas)...\n",
      "Processando lote 280/54150 (128 linhas)...\n",
      "Processando lote 281/54150 (128 linhas)...\n",
      "Processando lote 282/54150 (128 linhas)...\n",
      "Processando lote 283/54150 (128 linhas)...\n",
      "Processando lote 284/54150 (128 linhas)...\n",
      "Processando lote 285/54150 (128 linhas)...\n",
      "Processando lote 286/54150 (128 linhas)...\n",
      "Processando lote 287/54150 (128 linhas)...\n",
      "Processando lote 288/54150 (128 linhas)...\n",
      "Processando lote 289/54150 (128 linhas)...\n",
      "Processando lote 290/54150 (128 linhas)...\n",
      "Processando lote 291/54150 (128 linhas)...\n",
      "Processando lote 292/54150 (128 linhas)...\n",
      "Processando lote 293/54150 (128 linhas)...\n",
      "Processando lote 294/54150 (128 linhas)...\n",
      "Processando lote 295/54150 (128 linhas)...\n",
      "Processando lote 296/54150 (128 linhas)...\n",
      "Processando lote 297/54150 (128 linhas)...\n",
      "Processando lote 298/54150 (128 linhas)...\n",
      "Processando lote 299/54150 (128 linhas)...\n",
      "Processando lote 300/54150 (128 linhas)...\n",
      "Processando lote 301/54150 (128 linhas)...\n",
      "Processando lote 302/54150 (128 linhas)...\n",
      "Processando lote 303/54150 (128 linhas)...\n",
      "Processando lote 304/54150 (128 linhas)...\n",
      "Processando lote 305/54150 (128 linhas)...\n",
      "Processando lote 306/54150 (128 linhas)...\n",
      "Processando lote 307/54150 (128 linhas)...\n",
      "Processando lote 308/54150 (128 linhas)...\n",
      "Processando lote 309/54150 (128 linhas)...\n",
      "Processando lote 310/54150 (128 linhas)...\n",
      "Processando lote 311/54150 (128 linhas)...\n",
      "Processando lote 312/54150 (128 linhas)...\n",
      "Processando lote 313/54150 (128 linhas)...\n",
      "Processando lote 314/54150 (128 linhas)...\n",
      "Processando lote 315/54150 (128 linhas)...\n",
      "Processando lote 316/54150 (128 linhas)...\n",
      "Processando lote 317/54150 (128 linhas)...\n",
      "Processando lote 318/54150 (128 linhas)...\n",
      "Processando lote 319/54150 (128 linhas)...\n",
      "Processando lote 320/54150 (128 linhas)...\n",
      "Processando lote 321/54150 (128 linhas)...\n",
      "Processando lote 322/54150 (128 linhas)...\n",
      "Processando lote 323/54150 (128 linhas)...\n",
      "Processando lote 324/54150 (128 linhas)...\n",
      "Processando lote 325/54150 (128 linhas)...\n",
      "Processando lote 326/54150 (128 linhas)...\n",
      "Processando lote 327/54150 (128 linhas)...\n",
      "Processando lote 328/54150 (128 linhas)...\n",
      "Processando lote 329/54150 (128 linhas)...\n",
      "Processando lote 330/54150 (128 linhas)...\n",
      "Processando lote 331/54150 (128 linhas)...\n",
      "Processando lote 332/54150 (128 linhas)...\n",
      "Processando lote 333/54150 (128 linhas)...\n",
      "Processando lote 334/54150 (128 linhas)...\n",
      "Processando lote 335/54150 (128 linhas)...\n",
      "Processando lote 336/54150 (128 linhas)...\n",
      "Processando lote 337/54150 (128 linhas)...\n",
      "Processando lote 338/54150 (128 linhas)...\n",
      "Processando lote 339/54150 (128 linhas)...\n",
      "Processando lote 340/54150 (128 linhas)...\n",
      "Processando lote 341/54150 (128 linhas)...\n",
      "Processando lote 342/54150 (128 linhas)...\n",
      "Processando lote 343/54150 (128 linhas)...\n",
      "Processando lote 344/54150 (128 linhas)...\n",
      "Processando lote 345/54150 (128 linhas)...\n",
      "Processando lote 346/54150 (128 linhas)...\n",
      "Processando lote 347/54150 (128 linhas)...\n",
      "Processando lote 348/54150 (128 linhas)...\n",
      "Processando lote 349/54150 (128 linhas)...\n",
      "Processando lote 350/54150 (128 linhas)...\n",
      "Processando lote 351/54150 (128 linhas)...\n",
      "Processando lote 352/54150 (128 linhas)...\n",
      "Processando lote 353/54150 (128 linhas)...\n",
      "Processando lote 354/54150 (128 linhas)...\n",
      "Processando lote 355/54150 (128 linhas)...\n",
      "Processando lote 356/54150 (128 linhas)...\n",
      "Processando lote 357/54150 (128 linhas)...\n",
      "Processando lote 358/54150 (128 linhas)...\n",
      "Processando lote 359/54150 (128 linhas)...\n",
      "Processando lote 360/54150 (128 linhas)...\n",
      "Processando lote 361/54150 (128 linhas)...\n",
      "Processando lote 362/54150 (128 linhas)...\n",
      "Processando lote 363/54150 (128 linhas)...\n",
      "Processando lote 364/54150 (128 linhas)...\n",
      "Processando lote 365/54150 (128 linhas)...\n",
      "Processando lote 366/54150 (128 linhas)...\n",
      "Processando lote 367/54150 (128 linhas)...\n",
      "Processando lote 368/54150 (128 linhas)...\n",
      "Processando lote 369/54150 (128 linhas)...\n",
      "Processando lote 370/54150 (128 linhas)...\n",
      "Processando lote 371/54150 (128 linhas)...\n",
      "Processando lote 372/54150 (128 linhas)...\n",
      "Processando lote 373/54150 (128 linhas)...\n",
      "Processando lote 374/54150 (128 linhas)...\n",
      "Processando lote 375/54150 (128 linhas)...\n",
      "Processando lote 376/54150 (128 linhas)...\n",
      "Processando lote 377/54150 (128 linhas)...\n",
      "Processando lote 378/54150 (128 linhas)...\n",
      "Processando lote 379/54150 (128 linhas)...\n",
      "Processando lote 380/54150 (128 linhas)...\n",
      "Processando lote 381/54150 (128 linhas)...\n",
      "Processo finalizado.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 203\u001b[39m\n\u001b[32m    200\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProcesso finalizado.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 184\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    181\u001b[39m collection.load()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mColeção carregada.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m success = \u001b[43mprocess_and_insert_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTSV_FILEPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDados inseridos com sucesso. Deletando arquivo original: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTSV_FILEPATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mprocess_and_insert_data\u001b[39m\u001b[34m(collection, filepath, embedding_model)\u001b[39m\n\u001b[32m    106\u001b[39m body_texts = batch_df[REVIEW_BODY_FIELD].to_list()\n\u001b[32m    108\u001b[39m headline_embeddings = embedding_model.encode(headline_texts, show_progress_bar=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m body_embeddings = \u001b[43membedding_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m data_to_insert = []\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row_dict \u001b[38;5;129;01min\u001b[39;00m batch_df.to_dicts(): \u001b[38;5;66;03m# Iterar sobre as linhas do batch_df\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:685\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[39m\n\u001b[32m    682\u001b[39m features.update(extra_features)\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    687\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:758\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m    756\u001b[39m     module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m    757\u001b[39m     module_kwargs = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:442\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[32m    436\u001b[39m trans_features = {\n\u001b[32m    437\u001b[39m     key: value\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items()\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minputs_embeds\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    440\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m token_embeddings = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    444\u001b[39m features[\u001b[33m\"\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = token_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1016\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1012\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1013\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1014\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1016\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1028\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1029\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:662\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    651\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    652\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    653\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    659\u001b[39m         output_attentions,\n\u001b[32m    660\u001b[39m     )\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    672\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:552\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    541\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    542\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    549\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m    550\u001b[39m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[32m    551\u001b[39m     self_attn_past_key_value = past_key_value[:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    561\u001b[39m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:482\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    473\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    474\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    480\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    481\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    492\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\VDBMS\\venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:407\u001b[39m, in \u001b[36mBertSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[32m    400\u001b[39m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[32m    403\u001b[39m is_causal = (\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    405\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    417\u001b[39m attn_output = attn_output.reshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m.all_head_size)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"./Dados/\"\n",
    "\n",
    "# --- Configurações ---\n",
    "MILVUS_HOST = \"localhost\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "COLLECTION_NAME = \"reviews_data_collection\"\n",
    "ID_FIELD_NAME = \"pk_id\"\n",
    "REVIEW_HEADLINE_FIELD = \"review_headline\"\n",
    "REVIEW_BODY_FIELD = \"review_body\"\n",
    "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "EMBEDDING_DIMENSION = 384\n",
    "BATCH_SIZE = 128\n",
    "MAX_LEN_REVIEW_TEXT = 65530 # Um pouco menos que 65535 para segurança\n",
    "MAX_LEN_OTHER_VARCHAR = 1000 # Ajuste conforme necessário para suas outras colunas\n",
    "TSV_FILEPATH = DATA_DIR + \"amazon_reviews_multilingual_US_v1_00.tsv\" # !!! SUBSTITUA PELO CAMINHO DO SEU ARQUIVO TSV !!!\n",
    "\n",
    "def connect_to_milvus():\n",
    "    \"\"\"Conecta-se à instância Milvus.\"\"\"\n",
    "    print(f\"Conectando ao Milvus em {MILVUS_HOST}:{MILVUS_PORT}...\")\n",
    "    try:\n",
    "        connections.connect(alias=\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "        print(\"Conectado ao Milvus com sucesso!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Falha ao conectar ao Milvus: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_collection_if_not_exists(all_tsv_columns):\n",
    "    \"\"\"\n",
    "    Cria a coleção no Milvus se ela não existir.\n",
    "    O esquema incluirá um campo ID, os campos de texto originais,\n",
    "    seus campos de embedding correspondentes, e todos os outros campos do DataFrame.\n",
    "    \"\"\"\n",
    "    if utility.has_collection(COLLECTION_NAME):\n",
    "        print(f\"Coleção '{COLLECTION_NAME}' já existe.\")\n",
    "        return Collection(COLLECTION_NAME)\n",
    "\n",
    "    print(f\"Criando coleção '{COLLECTION_NAME}'...\")\n",
    "    \n",
    "    fields = [\n",
    "        FieldSchema(name=ID_FIELD_NAME, dtype=DataType.INT64, is_primary=True, auto_id=True, description=\"Primary key\"),\n",
    "        FieldSchema(name=REVIEW_HEADLINE_FIELD, dtype=DataType.VARCHAR, max_length=MAX_LEN_REVIEW_TEXT, description=\"Original review headline text\"),\n",
    "        FieldSchema(name=f\"{REVIEW_HEADLINE_FIELD}_embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIMENSION, description=\"Embedding of review headline\"),\n",
    "        FieldSchema(name=REVIEW_BODY_FIELD, dtype=DataType.VARCHAR, max_length=MAX_LEN_REVIEW_TEXT, description=\"Original review body text\"),\n",
    "        FieldSchema(name=f\"{REVIEW_BODY_FIELD}_embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIMENSION, description=\"Embedding of review body\")\n",
    "    ]\n",
    "\n",
    "    defined_field_names = {field.name for field in fields}\n",
    "    for col_name in all_tsv_columns:\n",
    "        if col_name not in defined_field_names:\n",
    "            fields.append(FieldSchema(name=col_name, dtype=DataType.VARCHAR, max_length=MAX_LEN_OTHER_VARCHAR, description=f\"Other data column: {col_name}\"))\n",
    "\n",
    "    schema = CollectionSchema(fields=fields, description=\"Collection for reviews data with embeddings for headline and body\")\n",
    "    collection = Collection(COLLECTION_NAME, schema=schema)\n",
    "    print(f\"Coleção '{COLLECTION_NAME}' criada com sucesso com o seguinte esquema:\")\n",
    "    for field in collection.schema.fields:\n",
    "        print(f\"  - {field.name}: {field.dtype}, Primary: {field.is_primary}, MaxLen: {getattr(field, 'params', {}).get('max_length', 'N/A')}\")\n",
    "    \n",
    "    print(\"Criando índices para os campos de embedding...\")\n",
    "    index_params = {\n",
    "        \"metric_type\": \"L2\", # Ou \"IP\" para produto interno\n",
    "        \"index_type\": \"IVF_FLAT\",\n",
    "        \"params\": {\"nlist\": 128},\n",
    "    }\n",
    "    collection.create_index(field_name=f\"{REVIEW_HEADLINE_FIELD}_embedding\", index_params=index_params)\n",
    "    collection.create_index(field_name=f\"{REVIEW_BODY_FIELD}_embedding\", index_params=index_params)\n",
    "    print(\"Índices criados.\")\n",
    "    \n",
    "    return collection\n",
    "\n",
    "def process_and_insert_data(collection, filepath, embedding_model):\n",
    "    \"\"\"Lê dados do arquivo, gera embeddings e insere no Milvus.\"\"\"\n",
    "    try:\n",
    "        df = pl.read_csv(filepath, separator='\\t', quote_char=None)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo '{filepath}' não encontrado.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler o arquivo TSV '{filepath}': {e}\")\n",
    "        return False\n",
    "\n",
    "    print(f\"Arquivo '{filepath}' carregado. Total de {df.height} linhas.\")\n",
    "    \n",
    "    if REVIEW_HEADLINE_FIELD not in df.columns or REVIEW_BODY_FIELD not in df.columns:\n",
    "        print(f\"Erro: As colunas '{REVIEW_HEADLINE_FIELD}' ou '{REVIEW_BODY_FIELD}' (ou ambas) não foram encontradas no TSV.\")\n",
    "        print(f\"Colunas disponíveis: {df.columns}\")\n",
    "        return False\n",
    "\n",
    "    # Lidar com NaNs nas colunas de texto antes do embedding e garantir que são strings\n",
    "    df = df.with_columns([\n",
    "        pl.col(REVIEW_HEADLINE_FIELD).fill_null(\"\").cast(pl.String),\n",
    "        pl.col(REVIEW_BODY_FIELD).fill_null(\"\").cast(pl.String)\n",
    "    ])\n",
    "    \n",
    "    print(f\"Processando e inserindo dados em lotes de {BATCH_SIZE}...\")\n",
    "    schema_field_names_for_insert = [field.name for field in collection.schema.fields if not field.is_primary]\n",
    "    num_batches = (df.height - 1) // BATCH_SIZE + 1\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_df = df.slice(i * BATCH_SIZE, BATCH_SIZE)\n",
    "        if batch_df.is_empty():\n",
    "            continue\n",
    "\n",
    "        print(f\"Processando lote {i+1}/{num_batches} ({batch_df.height} linhas)...\")\n",
    "\n",
    "        headline_texts = batch_df[REVIEW_HEADLINE_FIELD].to_list()\n",
    "        body_texts = batch_df[REVIEW_BODY_FIELD].to_list()\n",
    "\n",
    "        headline_embeddings = embedding_model.encode(headline_texts, show_progress_bar=False)\n",
    "        body_embeddings = embedding_model.encode(body_texts, show_progress_bar=False)\n",
    "\n",
    "        data_to_insert = []\n",
    "        for row_dict in batch_df.to_dicts(): # Iterar sobre as linhas do batch_df\n",
    "            current_row_values = []\n",
    "            \n",
    "            for field_name in schema_field_names_for_insert:\n",
    "                if field_name == f\"{REVIEW_HEADLINE_FIELD}_embedding\":\n",
    "                    current_row_values.append(headline_embeddings[data_to_insert.__len__() % BATCH_SIZE]) # Usar índice relativo ao lote\n",
    "                elif field_name == f\"{REVIEW_BODY_FIELD}_embedding\":\n",
    "                    current_row_values.append(body_embeddings[data_to_insert.__len__() % BATCH_SIZE]) # Usar índice relativo ao lote\n",
    "                elif field_name == REVIEW_HEADLINE_FIELD:\n",
    "                    text_val = str(row_dict.get(field_name, \"\"))\n",
    "                    current_row_values.append(text_val[:MAX_LEN_REVIEW_TEXT])\n",
    "                elif field_name == REVIEW_BODY_FIELD:\n",
    "                    text_val = str(row_dict.get(field_name, \"\"))\n",
    "                    current_row_values.append(text_val[:MAX_LEN_REVIEW_TEXT])\n",
    "                elif field_name in row_dict: # Outras colunas do TSV\n",
    "                    value = row_dict[field_name]\n",
    "                    # Polars usa None para missing, converter para string\n",
    "                    str_value = str(value) if value is not None else \"\"\n",
    "                    current_row_values.append(str_value[:MAX_LEN_OTHER_VARCHAR])\n",
    "                else:\n",
    "                    print(f\"Aviso: Campo '{field_name}' no esquema mas não nos dados da linha. Usando string vazia.\")\n",
    "                    current_row_values.append(\"\")\n",
    "            \n",
    "            data_to_insert.append(current_row_values)\n",
    "\n",
    "        try:\n",
    "            if data_to_insert: # Certifique-se de que há dados para inserir\n",
    "                # Transpor os dados do formato de lista de linhas para lista de colunas\n",
    "                # Milvus espera uma lista de colunas, onde cada coluna é uma lista de valores para esse campo.\n",
    "                # data_to_insert é List[List[Any]] (BATCH_SIZE linhas, 17 colunas)\n",
    "                # columnar_data será List[List[Any]] (17 colunas, BATCH_SIZE linhas)\n",
    "                columnar_data = [list(col) for col in zip(*data_to_insert)]\n",
    "                collection.insert(columnar_data)\n",
    "            else:\n",
    "                print(f\"Lote {i+1} não continha dados para inserir após o processamento.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao inserir o lote {i+1}: {e}\")\n",
    "             # Para depuração, você pode querer inspecionar as dimensões dos dados:\n",
    "            if 'columnar_data' in locals() and columnar_data:\n",
    "                print(f\"Debug: columnar_data tem {len(columnar_data)} colunas. A primeira coluna tem {len(columnar_data[0]) if columnar_data else 0} linhas.\")\n",
    "            elif data_to_insert:\n",
    "                print(f\"Debug: data_to_insert (antes da transposição) tem {len(data_to_insert)} linhas. A primeira linha tem {len(data_to_insert[0]) if data_to_insert else 0} campos.\")\n",
    "            return False # Retornar False em caso de falha na inserção\n",
    "\n",
    "    collection.flush()\n",
    "    print(f\"Todos os dados processados. Total de entidades na coleção '{COLLECTION_NAME}': {collection.num_entities}\")\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    print(f\"Carregando modelo de embedding '{EMBEDDING_MODEL_NAME}'...\")\n",
    "    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    print(\"Modelo de embedding carregado.\")\n",
    "\n",
    "    try:\n",
    "        connect_to_milvus()\n",
    "\n",
    "        try:\n",
    "            temp_df = pl.read_csv(TSV_FILEPATH, separator='\\t', n_rows=1, quote_char=None) \n",
    "            all_tsv_columns = temp_df.columns\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Erro: Arquivo TSV '{TSV_FILEPATH}' não encontrado. Verifique o caminho.\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Não foi possível ler as colunas do arquivo TSV '{TSV_FILEPATH}': {e}\")\n",
    "            return\n",
    "\n",
    "        collection = create_collection_if_not_exists(all_tsv_columns)\n",
    "        \n",
    "        print(f\"Carregando coleção '{COLLECTION_NAME}' na memória...\")\n",
    "        collection.load()\n",
    "        print(\"Coleção carregada.\")\n",
    "        \n",
    "        success = process_and_insert_data(collection, TSV_FILEPATH, embedding_model)\n",
    "\n",
    "        if success:\n",
    "            print(f\"Dados inseridos com sucesso. Deletando arquivo original: {TSV_FILEPATH}\")\n",
    "            try:\n",
    "                os.remove(TSV_FILEPATH)\n",
    "                print(f\"Arquivo '{TSV_FILEPATH}' deletado com sucesso.\")\n",
    "            except OSError as e:\n",
    "                print(f\"Erro ao deletar o arquivo '{TSV_FILEPATH}': {e}\")\n",
    "        else:\n",
    "            print(f\"Processo de inserção de dados do arquivo '{TSV_FILEPATH}' falhou. O arquivo não será deletado.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro geral no processo: {e}\")\n",
    "    finally:\n",
    "        # connections.disconnect(\"default\") # Descomente se desejar desconectar explicitamente\n",
    "        print(\"Processo finalizado.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf992ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pymilvus import connections, utility, Collection, DataType, FieldSchema, CollectionSchema\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "DATA_DIR = \"./Dados/\"\n",
    "\n",
    "# --- Configurações ---\n",
    "MILVUS_HOST = \"localhost\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "COLLECTION_NAME = \"reviews_data_collection\"\n",
    "ID_FIELD_NAME = \"pk_id\" # Este será nosso ID primário gerenciado manualmente\n",
    "REVIEW_HEADLINE_FIELD = \"review_headline\"\n",
    "REVIEW_BODY_FIELD = \"review_body\"\n",
    "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "EMBEDDING_DIMENSION = 384\n",
    "BATCH_SIZE = 128\n",
    "MAX_LEN_REVIEW_TEXT = 65530\n",
    "MAX_LEN_OTHER_VARCHAR = 1000\n",
    "TSV_FILEPATH = DATA_DIR + \"amazon_reviews_multilingual_US_v1_00.tsv\"\n",
    "PROGRESS_FILE = \"milvus_ingestion_progress.txt\"\n",
    "\n",
    "def connect_to_milvus():\n",
    "    \"\"\"Conecta-se à instância Milvus.\"\"\"\n",
    "    print(f\"Conectando ao Milvus em {MILVUS_HOST}:{MILVUS_PORT}...\")\n",
    "    try:\n",
    "        connections.connect(alias=\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "        print(\"Conectado ao Milvus com sucesso!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Falha ao conectar ao Milvus: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_collection_if_not_exists(all_tsv_columns):\n",
    "    \"\"\"\n",
    "    Cria a coleção no Milvus se ela não existir.\n",
    "    O ID primário será gerenciado manualmente.\n",
    "    \"\"\"\n",
    "    if utility.has_collection(COLLECTION_NAME):\n",
    "        print(f\"Coleção '{COLLECTION_NAME}' já existe.\")\n",
    "        collection = Collection(COLLECTION_NAME)\n",
    "        # Verificar se o esquema existente corresponde ao esperado (especialmente auto_id para o PK)\n",
    "        pk_field_schema = next((f for f in collection.schema.fields if f.name == ID_FIELD_NAME), None)\n",
    "        if pk_field_schema and pk_field_schema.auto_id:\n",
    "            print(f\"AVISO: A coleção '{COLLECTION_NAME}' existente tem auto_id=True para o campo '{ID_FIELD_NAME}'.\")\n",
    "            print(\"Para usar IDs gerenciados manualmente, a coleção pode precisar ser recriada com auto_id=False.\")\n",
    "            # Poderia adicionar lógica para dropar e recriar, mas é mais seguro alertar.\n",
    "        return collection\n",
    "\n",
    "    print(f\"Criando coleção '{COLLECTION_NAME}'...\")\n",
    "    \n",
    "    fields = [\n",
    "        FieldSchema(name=ID_FIELD_NAME, dtype=DataType.INT64, is_primary=True, auto_id=False, description=\"Primary key (managed manually)\"),\n",
    "        FieldSchema(name=REVIEW_HEADLINE_FIELD, dtype=DataType.VARCHAR, max_length=MAX_LEN_REVIEW_TEXT, description=\"Original review headline text\"),\n",
    "        FieldSchema(name=f\"{REVIEW_HEADLINE_FIELD}_embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIMENSION, description=\"Embedding of review headline\"),\n",
    "        FieldSchema(name=REVIEW_BODY_FIELD, dtype=DataType.VARCHAR, max_length=MAX_LEN_REVIEW_TEXT, description=\"Original review body text\"),\n",
    "        FieldSchema(name=f\"{REVIEW_BODY_FIELD}_embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIMENSION, description=\"Embedding of review body\")\n",
    "    ]\n",
    "\n",
    "    defined_field_names = {field.name for field in fields}\n",
    "    for col_name in all_tsv_columns:\n",
    "        if col_name not in defined_field_names:\n",
    "            fields.append(FieldSchema(name=col_name, dtype=DataType.VARCHAR, max_length=MAX_LEN_OTHER_VARCHAR, description=f\"Other data column: {col_name}\"))\n",
    "\n",
    "    schema = CollectionSchema(fields=fields, description=\"Collection for reviews data with manually managed PK and embeddings\")\n",
    "    collection = Collection(COLLECTION_NAME, schema=schema)\n",
    "    print(f\"Coleção '{COLLECTION_NAME}' criada com sucesso com o seguinte esquema:\")\n",
    "    for field in collection.schema.fields:\n",
    "        print(f\"  - {field.name}: {field.dtype}, Primary: {field.is_primary}, AutoID: {field.auto_id if hasattr(field, 'auto_id') else 'N/A'}, MaxLen: {getattr(field, 'params', {}).get('max_length', 'N/A')}\")\n",
    "    \n",
    "    print(\"Criando índices para os campos de embedding...\")\n",
    "    index_params = {\n",
    "        \"metric_type\": \"L2\",\n",
    "        \"index_type\": \"IVF_FLAT\",\n",
    "        \"params\": {\"nlist\": 128},\n",
    "    }\n",
    "    collection.create_index(field_name=f\"{REVIEW_HEADLINE_FIELD}_embedding\", index_params=index_params)\n",
    "    collection.create_index(field_name=f\"{REVIEW_BODY_FIELD}_embedding\", index_params=index_params)\n",
    "    print(\"Índices criados.\")\n",
    "    \n",
    "    return collection\n",
    "\n",
    "def process_and_insert_data(collection, filepath, embedding_model):\n",
    "    \"\"\"Lê dados, gera IDs e embeddings, e insere no Milvus.\"\"\"\n",
    "    try:\n",
    "        df_original = pl.read_csv(filepath, separator='\\t', quote_char=None)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo '{filepath}' não encontrado.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler o arquivo TSV '{filepath}': {e}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"Arquivo '{filepath}' carregado. Total de {df_original.height} linhas no arquivo original.\")\n",
    "\n",
    "    start_row_offset = 0\n",
    "    try:\n",
    "        with open(PROGRESS_FILE, \"r\") as f:\n",
    "            content = f.read().strip()\n",
    "            if content:\n",
    "                start_row_offset = int(content)\n",
    "                print(f\"Arquivo de progresso encontrado. Retomando a partir da linha offset: {start_row_offset}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Nenhum arquivo de progresso encontrado. Iniciando do começo.\")\n",
    "    except ValueError:\n",
    "        print(f\"Arquivo de progresso '{PROGRESS_FILE}' corrompido. Iniciando do começo.\")\n",
    "        start_row_offset = 0\n",
    "\n",
    "    if start_row_offset >= df_original.height:\n",
    "        print(\"Todos os dados já foram processados anteriormente conforme arquivo de progresso.\")\n",
    "        if os.path.exists(PROGRESS_FILE): os.remove(PROGRESS_FILE)\n",
    "        return True\n",
    "        \n",
    "    df_to_process = df_original.slice(start_row_offset, None)\n",
    "    \n",
    "    if REVIEW_HEADLINE_FIELD not in df_to_process.columns or REVIEW_BODY_FIELD not in df_to_process.columns:\n",
    "        print(f\"Erro: As colunas '{REVIEW_HEADLINE_FIELD}' ou '{REVIEW_BODY_FIELD}' não foram encontradas.\")\n",
    "        print(f\"Colunas disponíveis: {df_to_process.columns}\")\n",
    "        return False\n",
    "\n",
    "    df_to_process = df_to_process.with_columns([\n",
    "        pl.col(REVIEW_HEADLINE_FIELD).fill_null(\"\").cast(pl.String),\n",
    "        pl.col(REVIEW_BODY_FIELD).fill_null(\"\").cast(pl.String)\n",
    "    ])\n",
    "    \n",
    "    if df_to_process.is_empty():\n",
    "        print(\"Nenhum dado restante para processar (após considerar o progresso).\")\n",
    "        if start_row_offset >= df_original.height and os.path.exists(PROGRESS_FILE):\n",
    "             os.remove(PROGRESS_FILE)\n",
    "        return True\n",
    "        \n",
    "    print(f\"Processando {df_to_process.height} linhas restantes em lotes de {BATCH_SIZE}...\")\n",
    "    # Campos para inserir, EXCLUINDO o primário (que será adicionado primeiro)\n",
    "    schema_field_names_for_insert_non_pk = [field.name for field in collection.schema.fields if not field.is_primary]\n",
    "    num_batches = (df_to_process.height - 1) // BATCH_SIZE + 1\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_df = df_to_process.slice(i * BATCH_SIZE, BATCH_SIZE)\n",
    "        if batch_df.is_empty():\n",
    "            continue\n",
    "\n",
    "        current_batch_global_start_line = start_row_offset + (i * BATCH_SIZE)\n",
    "        print(f\"Processando lote {i+1}/{num_batches} (globalmente a partir da linha {current_batch_global_start_line}, {batch_df.height} linhas neste lote)...\")\n",
    "        \n",
    "        headline_texts = batch_df[REVIEW_HEADLINE_FIELD].to_list()\n",
    "        body_texts = batch_df[REVIEW_BODY_FIELD].to_list()\n",
    "\n",
    "        headline_embeddings = embedding_model.encode(headline_texts, show_progress_bar=False)\n",
    "        body_embeddings = embedding_model.encode(body_texts, show_progress_bar=False)\n",
    "        \n",
    "        ids_for_batch = []\n",
    "        data_for_non_pk_fields_rows = [] # Lista de linhas, cada linha é uma lista de valores para campos não-PK\n",
    "        \n",
    "        for idx_in_batch, row_dict in enumerate(batch_df.to_dicts()):\n",
    "            # Gerar ID primário: offset global da linha no arquivo original\n",
    "            pk_value = current_batch_global_start_line + idx_in_batch\n",
    "            ids_for_batch.append(pk_value)\n",
    "\n",
    "            current_row_non_pk_values = []\n",
    "            for field_name in schema_field_names_for_insert_non_pk:\n",
    "                if field_name == f\"{REVIEW_HEADLINE_FIELD}_embedding\":\n",
    "                    current_row_non_pk_values.append(headline_embeddings[idx_in_batch])\n",
    "                elif field_name == f\"{REVIEW_BODY_FIELD}_embedding\":\n",
    "                    current_row_non_pk_values.append(body_embeddings[idx_in_batch])\n",
    "                elif field_name == REVIEW_HEADLINE_FIELD:\n",
    "                    text_val = str(row_dict.get(field_name, \"\"))\n",
    "                    current_row_non_pk_values.append(text_val[:MAX_LEN_REVIEW_TEXT])\n",
    "                elif field_name == REVIEW_BODY_FIELD:\n",
    "                    text_val = str(row_dict.get(field_name, \"\"))\n",
    "                    current_row_non_pk_values.append(text_val[:MAX_LEN_REVIEW_TEXT])\n",
    "                elif field_name in row_dict:\n",
    "                    value = row_dict[field_name]\n",
    "                    str_value = str(value) if value is not None else \"\"\n",
    "                    current_row_non_pk_values.append(str_value[:MAX_LEN_OTHER_VARCHAR])\n",
    "                else:\n",
    "                    # print(f\"Aviso: Campo '{field_name}' no esquema mas não nos dados da linha. Usando string vazia.\")\n",
    "                    current_row_non_pk_values.append(\"\")\n",
    "            data_for_non_pk_fields_rows.append(current_row_non_pk_values)\n",
    "\n",
    "        try:\n",
    "            if ids_for_batch: # Se há IDs, deve haver dados\n",
    "                # Transpor os dados dos campos não primários para formato colunar\n",
    "                columnar_data_non_pk = [list(col) for col in zip(*data_for_non_pk_fields_rows)]\n",
    "                \n",
    "                # Montar os dados finais para inserção: [pk_column_data, non_pk_col1_data, ...]\n",
    "                # A ordem deve corresponder ao esquema da coleção.\n",
    "                final_columnar_data_to_insert = [ids_for_batch] + columnar_data_non_pk\n",
    "                \n",
    "                collection.insert(final_columnar_data_to_insert)\n",
    "                collection.flush()\n",
    "\n",
    "                processed_rows_in_this_batch_run = (i * BATCH_SIZE) + batch_df.height\n",
    "                next_global_start_row = start_row_offset + processed_rows_in_this_batch_run\n",
    "                try:\n",
    "                    with open(PROGRESS_FILE, \"w\") as f:\n",
    "                        f.write(str(next_global_start_row))\n",
    "                except IOError as e_progress:\n",
    "                    print(f\"Aviso: Não foi possível salvar o progresso após o lote {i+1}: {e_progress}\")\n",
    "            else:\n",
    "                print(f\"Lote {i+1} não continha dados para inserir após o processamento.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao inserir o lote {i+1} (índice de lote na execução atual): {e}\")\n",
    "            # Se a exceção for de ID duplicado (ex: pymilvus.exceptions.MilvusException e \"primary key ... already exist\" na msg)\n",
    "            # o script irá parar aqui, e o PROGRESS_FILE não será atualizado.\n",
    "            # Isso significa que na próxima execução, ele tentará este mesmo lote novamente.\n",
    "            return False\n",
    "\n",
    "    print(f\"Todos os dados processados. Total de entidades na coleção '{COLLECTION_NAME}': {collection.num_entities}\")\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        try:\n",
    "            os.remove(PROGRESS_FILE)\n",
    "            print(f\"Processamento concluído. Arquivo de progresso '{PROGRESS_FILE}' removido.\")\n",
    "        except OSError as e_rm_progress:\n",
    "            print(f\"Aviso: Não foi possível remover o arquivo de progresso '{PROGRESS_FILE}': {e_rm_progress}\")\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    print(f\"Carregando modelo de embedding '{EMBEDDING_MODEL_NAME}'...\")\n",
    "    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    print(\"Modelo de embedding carregado.\")\n",
    "\n",
    "    try:\n",
    "        connect_to_milvus()\n",
    "\n",
    "        try:\n",
    "            temp_df = pl.read_csv(TSV_FILEPATH, separator='\\t', n_rows=1, quote_char=None) \n",
    "            all_tsv_columns = temp_df.columns\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Erro: Arquivo TSV '{TSV_FILEPATH}' não encontrado. Verifique o caminho.\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Não foi possível ler as colunas do arquivo TSV '{TSV_FILEPATH}': {e}\")\n",
    "            return\n",
    "\n",
    "        collection = create_collection_if_not_exists(all_tsv_columns)\n",
    "        \n",
    "        print(f\"Carregando coleção '{COLLECTION_NAME}' na memória...\")\n",
    "        collection.load()\n",
    "        print(\"Coleção carregada.\")\n",
    "        \n",
    "        success = process_and_insert_data(collection, TSV_FILEPATH, embedding_model)\n",
    "\n",
    "        if success:\n",
    "            print(f\"Dados inseridos com sucesso. Deletando arquivo original: {TSV_FILEPATH}\")\n",
    "            try:\n",
    "                os.remove(TSV_FILEPATH)\n",
    "                print(f\"Arquivo '{TSV_FILEPATH}' deletado com sucesso.\")\n",
    "            except OSError as e:\n",
    "                print(f\"Erro ao deletar o arquivo '{TSV_FILEPATH}': {e}\")\n",
    "        else:\n",
    "            print(f\"Processo de inserção de dados do arquivo '{TSV_FILEPATH}' falhou. O arquivo não será deletado.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro geral no processo: {e}\")\n",
    "    finally:\n",
    "        # connections.disconnect(\"default\")\n",
    "        print(\"Processo finalizado.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
